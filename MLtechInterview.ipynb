{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yaxTV5iyhQ82"
   },
   "source": [
    "# ML Tech Interview\n",
    "\n",
    "Hello and welcome to the Machine Learning Tech Interview. This interview will be divided in two parts: the theoretical part and the practical/coding part. \n",
    "\n",
    "### **I will review only the scripts that will be sent (by pull request on this repo) by 5:00 pm on Friday**\n",
    "\n",
    "Good Luck!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HQjWqn__hQ85"
   },
   "source": [
    "## Theoretical Part\n",
    "\n",
    "Please answer the following questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-0X1MRGNhQ86"
   },
   "source": [
    "#### What are the assumptions of a linear model (or any other type of model)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9rLLjNphQ86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLinearity: The relationship between X and the mean of Y is linear.\\n??: The mean of residuals is zero (or very close to it).\\nHomoscedasticity: The variance of residual is the same for any value of X.\\n??: No autocorrelation of residuals.\\n??: The X variables and residuals are uncorrelated.\\nIndependence: Observations are independent of each other.\\n?? No perfect multicollinearity.\\nNormality: For any fixed value of X, Y is normally distributed.\\n??: Normality of residuals.\\n??: The number of observations must be greater than number of Xs.\\n??: The variability in X values is positive.\\n??: The regression model is correctly specified.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Linearity: The relationship between X and the mean of Y is linear.\n",
    "??: The mean of residuals is zero (or very close to it).\n",
    "Homoscedasticity: The variance of residual is the same for any value of X.\n",
    "??: No autocorrelation of residuals.\n",
    "??: The X variables and residuals are uncorrelated.\n",
    "Independence: Observations are independent of each other.\n",
    "?? No perfect multicollinearity.\n",
    "Normality: For any fixed value of X, Y is normally distributed.\n",
    "??: Normality of residuals.\n",
    "??: The number of observations must be greater than number of Xs.\n",
    "??: The variability in X values is positive.\n",
    "??: The regression model is correctly specified.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XWNx7P4HhQ8_"
   },
   "source": [
    "#### What’s the difference between K Nearest Neighbor and K-means Clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hQ16LsGthQ8_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nK Nearest Neighbor (KNN) is:\\n- supervised \\n- for classification or regression \\n- requires labeled data to train on\\n\\nK-means Clustering is:\\n- unsupervised \\n- for clustering\\n- iteratively partitions data points into clusters\\n\\nthe k value represents a different parameter as well.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "K Nearest Neighbor (KNN) is:\n",
    "- supervised \n",
    "- for classification or regression \n",
    "- requires labeled data to train on\n",
    "\n",
    "K-means Clustering is:\n",
    "- unsupervised \n",
    "- for clustering\n",
    "- iteratively partitions data points into clusters\n",
    "\n",
    "the k value represents a different parameter as well.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iDDPRfBRhQ9B"
   },
   "source": [
    "#### How do you address overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h1TyBsLwhQ9D"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOverfitting is the production of a model that corresponds too closely to a particular set of\\ndata, and may therefore fail to predict additional data. This usually occurs when a model has\\ntoo many features for the amount of data. Overfitting can be detected when our model does \\nmuch better on the training set than on the test set. There are several ways to address \\noverfitting:\\n\\n- Feature selection: dropping columns.\\n- Dimensionality reduction: techniques such as Principal Component Analysis (PCA) reduce the\\ndimensions of your feature space, hence you have fewer relationships between variables to \\nconsider and you are less likely to overfit your model.\\n- More data points\\n- Cross validation\\n- Regularization: these are techniques that attempt to simplify the model such as adding \\npenalties to the cost function.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Overfitting is the production of a model that corresponds too closely to a particular set of\n",
    "data, and may therefore fail to predict additional data. This usually occurs when a model has\n",
    "too many features for the amount of data. Overfitting can be detected when our model does \n",
    "much better on the training set than on the test set. There are several ways to address \n",
    "overfitting:\n",
    "\n",
    "- Feature selection: dropping columns.\n",
    "- Dimensionality reduction: techniques such as Principal Component Analysis (PCA) reduce the\n",
    "dimensions of your feature space, hence you have fewer relationships between variables to \n",
    "consider and you are less likely to overfit your model.\n",
    "- More data points\n",
    "- Cross validation\n",
    "- Regularization: these are techniques that attempt to simplify the model such as adding \n",
    "penalties to the cost function.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0dInW-RQhQ9G"
   },
   "source": [
    "#### Explain Naive Bayes algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U76IQ8xdhQ9H"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNaive Bayes (NB) is a classification algorithm. \\n\\nIt asks the question: “Given these features, does this measurement belong to class A or B?”,\\nthen answers it by taking the proportion of all previous measurements with the same features\\nbelonging to class A and multiplying it by the the proportion of all measurements in class A.\\nIf this number is bigger than the corresponding calculation for class B we say the \\nmeasurement belongs in class A. \\n\\nSince we\\'re assuming that all features are independent of each other, we can take each \\nfeature separately and determine the proportion of previous measurements that belong to \\nclass A that have the same value for this feature. Then we repeat this process to every other\\nfeature, and take its product, before finally multiplying it by the proportion of class A in \\nthe dataset. If this number is larger than when we do the corresponding calculation for class\\nB, then it belongs in class A.\\n\\nThis assumption of independence of features (which is almost never true), is what allows us \\nto not need to rely on exact duplicates from previously measured points, which would make \\npractically useless — as it would only be able to classify these duplicates and nothing more.\\n\\nThat\\'s why it\\'s called \\'naive\\'.\\n\\nThe great thing about NB is that this \"naive assumption\" actually tends to help the \\nclassification. The reason for this is that assuming two features are independent (when \\nthey\\'re actually dependent), implies we\\'re doubly-counting evidence. \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Naive Bayes (NB) is a classification algorithm. \n",
    "\n",
    "It asks the question: “Given these features, does this measurement belong to class A or B?”,\n",
    "then answers it by taking the proportion of all previous measurements with the same features\n",
    "belonging to class A and multiplying it by the the proportion of all measurements in class A.\n",
    "If this number is bigger than the corresponding calculation for class B we say the \n",
    "measurement belongs in class A. \n",
    "\n",
    "Since we're assuming that all features are independent of each other, we can take each \n",
    "feature separately and determine the proportion of previous measurements that belong to \n",
    "class A that have the same value for this feature. Then we repeat this process to every other\n",
    "feature, and take its product, before finally multiplying it by the proportion of class A in \n",
    "the dataset. If this number is larger than when we do the corresponding calculation for class\n",
    "B, then it belongs in class A.\n",
    "\n",
    "This assumption of independence of features (which is almost never true), is what allows us \n",
    "to not need to rely on exact duplicates from previously measured points, which would make \n",
    "practically useless — as it would only be able to classify these duplicates and nothing more.\n",
    "\n",
    "That's why it's called 'naive'.\n",
    "\n",
    "The great thing about NB is that this \"naive assumption\" actually tends to help the \n",
    "classification. The reason for this is that assuming two features are independent (when \n",
    "they're actually dependent), implies we're doubly-counting evidence. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UsLyyonHYwN"
   },
   "source": [
    "#### When do you use an AUC-ROC score? What kind of information can you gather from it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jp5SotUdHrZf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAUC (Area Under The Curve) ROC (Receiver Operating Characteristics). Can also written as \\nAUROC (Area Under the Receiver Operating Characteristics).\\n\\nIt's an evaluation metric for checking any classification model’s performance. \\n\\nIt's a performance measurement for classification problem at various thresholds settings. \\nROC is a probability curve and AUC represents degree or measure of separability. It tells how\\nmuch a model is capable of distinguishing between classes. Higher the AUC, better the model \\nis at predicting 0s as 0s and 1s as 1s. For example, the higher the AUC, the better the model\\nis at distinguishing between patients with disease and no disease.\\n\\nAn excellent model has AUC near to the 1 which means it has good measure of separability. A \\npoor model has AUC near to the 0 which means it has worst measure of separability. In fact it\\nmeans it is reciprocating the result: predicting 0s as 1s and 1s as 0s. And when AUC is 0.5,\\nit means model has no class separation capacity whatsoever.\\n\\nROC is a curve of probability. As such, when the two curves (0's and 1's) don’t overlap at \\nall, the model has an ideal measure of separability. In other words, it's perfectly able to \\ndistinguish between positive class and negative class.\\n\\nWhen two distributions overlap, we introduce type 1 and type 2 error. Depending upon the \\nthreshold, we can minimize or maximize them. When, for example, AUC is 0.7, it means there is\\na 70% chance that model will be able to distinguish between positive class and negative \\nclass.\\n\\nWhen AUC is approximately 0, model is actually reciprocating the classes. It means, model is\\npredicting negative class as a positive class and vice versa.\\n\\nFrom the AUC-ROC you can gather TPR (True Positive Rate)/Recall/Sensitivity, Specificity\\nand FPR (False Positive Rate), where:\\n\\nTPR/Recall/Sensitivity = TP / (TP + FN)\\n\\nwhile TP stands for True Positives and FN stands for False Negatives.\\n\\nSpecifity = TN / (TN + FP)\\n\\nwhile TN stands for True Positives and FP for False Positives.\\n\\nFPR = 1 - Specificity.\\n\\nSensitivity and Specificity are inversely proportional to each other. So when we increase \\nSensitivity, Specificity decreases and vice versa.\\n\\nWhen we decrease the threshold, we get more positive values thus it increases the Sensitivity\\nand decreases the Specificity.\\n\\nSimilarly, when we increase the threshold, we get more negative values thus we get higher \\nSpecificity and lower Sensitivity.\\n\\nSince FPR is 1 - Specificity, when we increase TPR, FPR also increases and vice versa.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "AUC (Area Under The Curve) ROC (Receiver Operating Characteristics). Can also written as \n",
    "AUROC (Area Under the Receiver Operating Characteristics).\n",
    "\n",
    "It's an evaluation metric for checking any classification model’s performance. \n",
    "\n",
    "It's a performance measurement for classification problem at various thresholds settings. \n",
    "ROC is a probability curve and AUC represents degree or measure of separability. It tells how\n",
    "much a model is capable of distinguishing between classes. Higher the AUC, better the model \n",
    "is at predicting 0s as 0s and 1s as 1s. For example, the higher the AUC, the better the model\n",
    "is at distinguishing between patients with disease and no disease.\n",
    "\n",
    "An excellent model has AUC near to the 1 which means it has good measure of separability. A \n",
    "poor model has AUC near to the 0 which means it has worst measure of separability. In fact it\n",
    "means it is reciprocating the result: predicting 0s as 1s and 1s as 0s. And when AUC is 0.5,\n",
    "it means model has no class separation capacity whatsoever.\n",
    "\n",
    "ROC is a curve of probability. As such, when the two curves (0's and 1's) don’t overlap at \n",
    "all, the model has an ideal measure of separability. In other words, it's perfectly able to \n",
    "distinguish between positive class and negative class.\n",
    "\n",
    "When two distributions overlap, we introduce type 1 and type 2 error. Depending upon the \n",
    "threshold, we can minimize or maximize them. When, for example, AUC is 0.7, it means there is\n",
    "a 70% chance that model will be able to distinguish between positive class and negative \n",
    "class.\n",
    "\n",
    "When AUC is approximately 0, model is actually reciprocating the classes. It means, model is\n",
    "predicting negative class as a positive class and vice versa.\n",
    "\n",
    "From the AUC-ROC you can gather TPR (True Positive Rate)/Recall/Sensitivity, Specificity\n",
    "and FPR (False Positive Rate), where:\n",
    "\n",
    "TPR/Recall/Sensitivity = TP / (TP + FN)\n",
    "\n",
    "while TP stands for True Positives and FN stands for False Negatives.\n",
    "\n",
    "Specifity = TN / (TN + FP)\n",
    "\n",
    "while TN stands for True Positives and FP for False Positives.\n",
    "\n",
    "FPR = 1 - Specificity.\n",
    "\n",
    "Sensitivity and Specificity are inversely proportional to each other. So when we increase \n",
    "Sensitivity, Specificity decreases and vice versa.\n",
    "\n",
    "When we decrease the threshold, we get more positive values thus it increases the Sensitivity\n",
    "and decreases the Specificity.\n",
    "\n",
    "Similarly, when we increase the threshold, we get more negative values thus we get higher \n",
    "Specificity and lower Sensitivity.\n",
    "\n",
    "Since FPR is 1 - Specificity, when we increase TPR, FPR also increases and vice versa.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6gMzc684hQ9J"
   },
   "source": [
    "#### What is cross validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yhBiZyJhQ9K"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cross validation (CV) is one of the technique used to test the effectiveness of machine \n",
    "learning models. It is also a re-sampling procedure used to evaluate a model if we have \n",
    "limited data. \n",
    "\n",
    "To perform CV we need to keep aside a sample of the data on which the model will be trained.\n",
    "This sample will not be seen by the model during trainning, and will instead be used latter \n",
    "for testing/validating. \n",
    "\n",
    "A few (common) CV techniques are: Train_Test Split and K-Folds Cross Validation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wqEQlGL2hQ9L"
   },
   "source": [
    "#### What are confounding variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-8POEd6hQ9M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ixSZaNdhQ9P"
   },
   "source": [
    "#### If an important metric for our company stopped appearing in our data source, how would you investigate the causes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOPUF1i3hQ9Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ko6vUaXshQ9S"
   },
   "source": [
    "## Practical Machine Learning\n",
    "\n",
    "In this challenge, you will showcase your knowledge in feature engineering, dimensionality reduction, model selection and evaluation, hyperparameter tuning, and any other techniques of machine learning.\n",
    "\n",
    "There isn't a correct solution to this challenge. All we would like to learn is your thinking process that demonstrates your knowledge, experience, and creativity in developing machine learning models. Therefore, in addition to developing the model and optimizing its performance, you should also elaborate your thinking process and justify your decisions thoughout the iterative problem-solving process.\n",
    "\n",
    "The suggested time to spend on this challenge is 90-120 minutes. If you don't have time to finish all the tasks you plan to do, simply document the to-dos at the end of your response.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "- Download the housing prices data set (housing_prices.csv). The data is big enough to showcase your thoughts but not so that processing power will be a problem.\n",
    "- Using Python, analyze the features and determine which feature set to select for modeling.\n",
    "- Train and cross validate several regression models, attempting to accurately predict the SalePrice target variable.\n",
    "- Evaluate all models and show comparison of performance metrics.\n",
    "- State your thoughts on model performance, which model(s) you would select, and why.\n",
    "\n",
    "#### Deliverables Checklist:\n",
    "\n",
    "- Python code.\n",
    "- Your thinking process.\n",
    "- The features selected for machine learning.\n",
    "- The results (e.g., performance metrics) of your selected model(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3moKc9n4hQ9T"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MLtechInterview.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
